{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbe74b1",
   "metadata": {},
   "source": [
    "# Exercise: Fine-Tuning FLUX.1-dev with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99538f87",
   "metadata": {},
   "source": [
    "app: https://brix--heroicons-flux-fastapi-app.modal.run/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c43b4f",
   "metadata": {},
   "source": [
    "The second half of the exercise was to fine-tune the hyperparameters, but unfortunately we did not have the compute to do that...\n",
    "\n",
    "\n",
    "Complete this table (default values):\n",
    "\n",
    "Configuration \t                        \n",
    "Learning rate + long training: 2e-4 / 4000\n",
    "Rank: 16\n",
    "\n",
    "- Why might lower ranks produce noisier outputs?\n",
    "LoRA rank controls the number of directions a model can adapt during fine-tuning. Lower ranks have fewer directions, limiting the modelâ€™s ability to capture complex patterns. This underfitting can make outputs appear noisy, blurry, or imprecise. Higher ranks provide more capacity, producing sharper and more stable outputs. The tradeoff is that low rank uses less memory and regularizes the model, while high rank can overfit if the data is small.\n",
    "\n",
    "- What signs indicate your model is overfitting?\n",
    "\n",
    "Overfitting occurs when a model memorizes training data instead of learning general patterns. Outputs on new data may look unrealistic, inconsistent, or overly sensitive to small input changes... A key sign indicating overfitting is much lower training loss than validation loss, showing poor generalization. Validation metrics may plateau or worsen while training metrics keep improving. Finally, extending training steps can improve training performance but reduce validation performance.\n",
    "\n",
    "- How would you adapt these settings for a different style (e.g., photorealistic portraits)?\n",
    "To adapt these settings for photorealistic portraits, I would adjust parameters to match the complexity and resolution of the new data. I would go with a lower learning rate, longer training and a higher rank. I would try them out using sweep parameters (which define values to try across multiple runs) with wandb to generate a separate fine-tuning run for each combination of parameters.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
